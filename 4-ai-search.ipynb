{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2406d69d",
   "metadata": {},
   "source": [
    "# ベクトル検索の実装 (Amazon Bedrock 編)<a id=\"vector-search-with-sagemaker\"></a>\n",
    "\n",
    "> この章は、`vector-search-with-sagemaker.ipynb` を元に作成しています。\n",
    "\n",
    "## 概要\n",
    "\n",
    "本ラボでは、Amazon Bedrock の密ベクトル埋め込みモデルを活用したベクトル検索を実装していきます。\n",
    "\n",
    "### ベクトル\n",
    "\n",
    "ベクトル検索とは、与えられたクエリアイテムに類似または関連するアイテムを効率的かつ効果的に検索する手法です。ベクトル間の距離や角度の近さといった数値に基づき、類似のアイテムやエンティティを探査します。従来の検索エンジンが苦手とする類似表現や関連語を含むクエリによる問い合わせでも類似性の高い結果を返すことができるため、レコメンドや類似検索、検索検索拡張生成(RAG) に代表される文書検索・ナレッジ検索で幅広く活用されています。\n",
    "\n",
    "全文検索はクエリと検索対象のデータ間で厳密なマッチングが要求される一方、ベクトル検索は\"意味的に近い\" 文書を取得する際に有用であるため、うまく使い分けることで幅広い検索要件を達成できます。\n",
    "\n",
    "一般的にベクトル検索とは、N 次元の数値配列からなる密ベクトルを使った検索のことを指します。密ベクトル検索では、クエリと検索対象のデータは N 次元の数値配列として扱われ、それらの距離や角度の差異が類似度として表されます。距離や角度が近いほど類似度が高いとみなすことができます。\n",
    "\n",
    "<img src=\"./img/dense-vector-search.png\" width=\"1024\">\n",
    "\n",
    "### ベクトル埋め込み\n",
    "\n",
    "ベクトル検索を行う上では、検索対象のテキストやクエリ文字列をベクトルデータに変換し、格納する必要があります。\n",
    "\n",
    "<img src=\"./img/dense-vector-embedding.png\" width=\"1024\">\n",
    "\n",
    "データをベクトルに変換する処理を \"埋め込み (Embedding)\" と呼びます。埋め込み処理は、一般的に機械学習モデルの一種である埋め込みモデル (Embedding model) によって生成します。<br>\n",
    "本ラボでは、Bedrock の `Amazon Titan Text Embeddings V2` を用いて、テキストからベクトルデータを生成します。モデルの詳細については [Hugging Face 上の解説](https://huggingface.co/amazon/Titan-text-embeddings-v2) を参照してください。\n",
    "\n",
    "### k-NN search\n",
    "\n",
    "OpenSearch においては、ベクトル検索を実行するために [k-NN search(k-nearest neighbors search)][knn] と呼ばれる機能を提供しています。k-NN search は、ベクトル空間内で最も近い k 個の近傍点を探す機能です。<br>\n",
    "k-NN search では、データセットの規模や要件に応じた複数の方式を提供しています。大規模データには Approximate k-NN、フィルタリングが必要な小規模データには Script Score k-NN、複雑なスコアリングが必要な場合は Painless extensions が推奨されてます。\n",
    "\n",
    "- Approximate k-NN：大規模データセット向けの近似検索方式です。インデックス作成速度と検索精度を多少犠牲にする代わりに、低レイテンシーと少ないメモリ使用量を実現します。\n",
    "- Script Score k-NN：完全一致の総当たり検索を行う方式です。フィルタリングと組み合わせた検索が可能です。小規模データセット向きです。\n",
    "- Painless extensions：距離関数をPainlessスクリプトの拡張として提供し、より複雑なスコアリングが可能です。\n",
    "\n",
    "本ラボでは、実ユースケースでも多く採用されている Approximate kNN を使用して k-NN search を実行していきます。\n",
    "\n",
    "### Approximate k-NN\n",
    "\n",
    "[Approximate k-NN](https://opensearch.org/docs/latest/search-plugins/knn/approximate-knn/) (近似最近傍探索、ANN)は、大規模なデータセットで効率的な類似検索を実現するための手法です。Script score による厳密な k-NN search はクエリと全てのデータポイント間の距離を総当たりで計算するため、高次元の大規模データセットでは処理効率が低下します。<br>\n",
    "ANN は、グラフやバケットなど独自のデータ構造にベクトルデータを格納することで、検索速度を大幅に向上させるアプローチです。精度が若干低下するものの、大規模なベクトルデータに対して効率的な検索が可能になります。\n",
    "\n",
    "OpenSearch では、ANN を実行するためのエンジンを以下 3 つ用意しています。ただし nmslib は将来廃止予定であるため、実質的には Faiss と Lucene のどちらかから選択する形となります。Faiss は高機能かつ高速であり、大規模データセットに適しています。Lucene は省リソースが特徴で、数百万ベクトルまでの小規模なデータセットで良好な性能を発揮します。\n",
    "\n",
    "- Faiss (デフォルト)\n",
    "- Lucene\n",
    "- nmslib (将来廃止予定)\n",
    "\n",
    "本ラボでは、実ユースケースでも多く採用されている Faiss エンジンを使用します。\n",
    "\n",
    "もう一つ ANN を使用するうえで必要になるのがアルゴリズムの選定です。OpenSearch では、以下 2 つのアルゴリズムを提供しています。HNSW は多くのメモリを必要としますが、高速な検索が可能です。IVF はメモリ効率が良好ですが、検索にあたっては事前トレーニングが必要です。各アルゴリズムの詳細については、AWS Bigdata blog の [OpenSearch における 10 億規模のユースケースに適した k-NN アルゴリズムの選定\n",
    "](https://aws.amazon.com/jp/blogs/news/choose-the-k-nn-algorithm-for-your-billion-scale-use-case-with-opensearch/)に詳しい解説が掲載されています。\n",
    "\n",
    "- HNSW (Hierarchical Navigable Small World)\n",
    "- IVF (Inverted File Index)\n",
    "\n",
    "本ラボでは、トレーニングが不要な HNSW を使用します。\n",
    "\n",
    "### OpenSearch におけるベクトル検索の流れ\n",
    "\n",
    "OpenSearch におけるベクトル検索の流れは以下の通りです。\n",
    "\n",
    "**ベクトルデータの登録**\n",
    "1. ドキュメントデータをもとに埋め込みモデルを呼び出し、ベクトルを作成\n",
    "1. ベクトル情報と元のドキュメントデータを検索インデックスに登録\n",
    "\n",
    "**検索**\n",
    "1. ユーザーから入力されたクエリをもとに埋め込みモデルを呼び出し、ベクトルを作成\n",
    "1. ベクトル情報を元にベクトル検索のクエリを組み立て、検索インデックスにベクトル検索のクエリを発行\n",
    "\n",
    "<img src=\"./img/dense-vector-embedding-overview.png\" width=\"1024\">\n",
    "\n",
    "### ラボの構成\n",
    "\n",
    "本ラボでは、ノートブック環境（EC2 へ Remote Develop 接続した VSCode）および Amazon OpenSearch Serverless、Amazon Bedrock を使用します。\n",
    "\n",
    "<img src=\"./img/architecture-with-bedrock.png\" width=\"50%\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "### 使用するデータセット\n",
    "\n",
    "本ラボでは、[JGLUE][jglue] 内の FAQ データセットである [JSQuAD][jsquad] を使用します。\n",
    "\n",
    "[jglue]: https://github.com/yahoojapan/JGLUE/tree/main\n",
    "[jsquad]: https://github.com/yahoojapan/JGLUE/tree/main/datasets/jsquad-v1.3\n",
    "[bge-m3]: https://huggingface.co/BAAI/bge-m3\n",
    "[knn]: https://opensearch.org/docs/latest/search-plugins/knn/index/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f592ac",
   "metadata": {},
   "source": [
    "## 事前作業\n",
    "\n",
    "### パッケージインストール\n",
    "実行する前に、タブの右上のカーネルの選択を確認してください。\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> \n",
    "opensearch と awswrangler のバージョンの依存関係のため、opensearch-py 側を止めています。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434200e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv add \"opensearch-py<3\" requests-aws4auth awswrangler[opensearch] tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbd1161",
   "metadata": {},
   "source": [
    "### 環境変数\n",
    "`.env`ファイルに、以下の環境変数を設定します。\n",
    "\n",
    "- AOSS_SEARCH_HOST=`AOSS の検索コレクションのエンドポイントのホスト名`\n",
    "- AOSS_VECTOR_HOST=`AOSS のベクトル検索コレクションのエンドポイントのホスト名`\n",
    "- AOSS_ROLE_ARN=`AOSS から Bedrock へアクセスするために作成したロールのARN`\n",
    "\n",
    "設定できたら、以下のコマンドを実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53c06f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv -o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b09bdb8",
   "metadata": {},
   "source": [
    "### インポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82a27c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import awswrangler as wr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3d9fa0",
   "metadata": {},
   "source": [
    "### 共通変数のセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43825d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_region = boto3.Session().region_name\n",
    "logging.getLogger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dea62a",
   "metadata": {},
   "source": [
    "## リソースの準備\n",
    "\n",
    "### Amazon Bedrock 関連リソースの準備\n",
    "\n",
    "#### 埋め込みモデルの準備\n",
    "\n",
    "ここまで、モデルの許可のみ\n",
    "\n",
    "##### 推論エンドポイントのテスト呼び出し\n",
    "\n",
    "[このコード](https://docs.aws.amazon.com/ja_jp/bedrock/latest/userguide/bedrock-runtime_example_bedrock-runtime_InvokeModelWithResponseStream_TitanTextEmbeddings_section.html)を参考に実装する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dc6e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Bedrock Runtime client in the AWS Region of your choice.\n",
    "bedrock_runtime_client = boto3.client(\"bedrock-runtime\", region_name=default_region)\n",
    "\n",
    "# Set the model ID, e.g., Titan Text Embeddings V2.\n",
    "model_id = \"amazon.titan-embed-text-v2:0\"\n",
    "\n",
    "# The text to convert to an embedding.\n",
    "input_text = \"Please recommend books with a theme similar to the movie 'Inception'.\"\n",
    "\n",
    "# Create the request for the model.\n",
    "native_request = {\"inputText\": input_text}\n",
    "\n",
    "# Convert the native request to JSON.\n",
    "request = json.dumps(native_request)\n",
    "\n",
    "# Invoke the model with the request.\n",
    "response = bedrock_runtime_client.invoke_model(modelId=model_id, body=request)\n",
    "\n",
    "# Decode the model's native response body.\n",
    "model_response = json.loads(response[\"body\"].read())\n",
    "\n",
    "# Extract and print the generated embedding and the input text token count.\n",
    "embedding = model_response[\"embedding\"]\n",
    "input_token_count = model_response[\"inputTextTokenCount\"]\n",
    "\n",
    "print(\"\\nYour input:\")\n",
    "print(input_text)\n",
    "print(f\"Number of input tokens: {input_token_count}\")\n",
    "print(f\"Size of the generated embedding: {len(embedding)}\")\n",
    "print(\"Embedding:\")\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d29c516",
   "metadata": {},
   "source": [
    "### サンプルデータの読み込み\n",
    "\n",
    "サンプルデータをダウンロードし、Pandas の DataFrame 形式に変換します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1af3982",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataset_dir = \"./dataset/jsquad/\"\n",
    "%mkdir -p $dataset_dir\n",
    "!curl -L -s -o $dataset_dir/valid.json https://github.com/yahoojapan/JGLUE/raw/main/datasets/jsquad-v1.3/valid-v1.3.json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4404909",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def squad_json_to_dataframe(input_file_path, record_path=[\"data\", \"paragraphs\", \"qas\", \"answers\"]):\n",
    "    file = json.loads(open(input_file_path).read())\n",
    "    m = pd.json_normalize(file, record_path[:-1])\n",
    "    r = pd.json_normalize(file, record_path[:-2])\n",
    "\n",
    "    idx = np.repeat(r[\"context\"].values, r.qas.str.len())\n",
    "    m[\"context\"] = idx\n",
    "    m[\"answers\"] = m[\"answers\"]\n",
    "    m[\"answers\"] = m[\"answers\"].apply(lambda x: np.unique(pd.json_normalize(x)[\"text\"].to_list()))\n",
    "    return m[[\"id\", \"question\", \"context\", \"answers\"]]\n",
    "\n",
    "valid_filename = f\"{dataset_dir}/valid.json\"\n",
    "valid_df = squad_json_to_dataframe(valid_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e1fc2b",
   "metadata": {},
   "source": [
    "### サンプルデータの確認\n",
    "\n",
    "サンプルデータは質問文フィールドの question、回答の answers、説明文の context フィールド、問題 ID である id フィールドから構成されています。<br>\n",
    "サンプルデータの一部を見ていきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db902bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff94a21",
   "metadata": {},
   "source": [
    "### OpenSearch Serverless への接続確認\n",
    "\n",
    "OpenSearch Server のセキュリティ設定により、API リクエストが許可されているかを確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b02293a",
   "metadata": {},
   "outputs": [],
   "source": [
    "aoss_host = os.getenv(\"AOSS_VECTOR_HOST\")\n",
    "\n",
    "credentials = boto3.Session().get_credentials()\n",
    "service_code = \"aoss\"\n",
    "auth = AWSV4SignerAuth(credentials=credentials, region=default_region, service=service_code)\n",
    "opensearch_client = OpenSearch(\n",
    "    hosts=[{\"host\": aoss_host, \"port\": 443}],\n",
    "    http_compress=True, \n",
    "    http_auth=auth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class = RequestsHttpConnection\n",
    ")\n",
    "opensearch_client.cat.indices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f0fefe",
   "metadata": {},
   "source": [
    "#### インデックスの作成\n",
    "\n",
    "id、question、context、answers フィールドを格納するための文字列型フィールドに加えて、question、context フィールドから生成したベクトルデータを格納するための context_embedding、question_embedding フィールドを持つインデックスを作成します。<br>\n",
    "question、context、answers フィールドについては、テキスト検索でもある程度の検索精度を出せるように、id フィールドを除いて kuromoji のカスタムアナライザーをセットしています。<br>\n",
    "OpenSearch では、ベクトルデータを格納するためのフィールドタイプとして knn_vector タイプを提供しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a789a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"id\": {\"type\": \"keyword\"},\n",
    "      \"question\": {\"type\": \"text\", \"analyzer\": \"custom_kuromoji_analyzer\"},\n",
    "      \"context\":  {\"type\": \"text\", \"analyzer\": \"custom_kuromoji_analyzer\"},\n",
    "      \"answers\":  {\"type\": \"text\", \"analyzer\": \"custom_kuromoji_analyzer\"},\n",
    "      \"question_embedding\": {\n",
    "        \"type\": \"knn_vector\",\n",
    "        \"dimension\": 1024,\n",
    "        \"space_type\": \"l2\",\n",
    "        \"method\": {\n",
    "          \"name\": \"hnsw\",\n",
    "          \"engine\": \"faiss\",\n",
    "        }\n",
    "      },\n",
    "      \"context_embedding\": {\n",
    "        \"type\": \"knn_vector\",\n",
    "        \"dimension\": 1024,\n",
    "        \"space_type\": \"l2\",\n",
    "        \"method\": {\n",
    "          \"name\": \"hnsw\",\n",
    "          \"engine\": \"faiss\",\n",
    "        },\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"settings\": {\n",
    "    \"index.knn\": True,\n",
    "    \"analysis\": {\n",
    "      \"analyzer\": {\n",
    "        \"custom_kuromoji_analyzer\": {\n",
    "          \"char_filter\": [\"icu_normalizer\"],\n",
    "          \"filter\": [\n",
    "              \"kuromoji_baseform\",\n",
    "              \"custom_kuromoji_part_of_speech\"\n",
    "          ],\n",
    "          \"tokenizer\": \"kuromoji_tokenizer\",\n",
    "          \"type\": \"custom\"\n",
    "        }\n",
    "      },\n",
    "      \"filter\": {\n",
    "        \"custom_kuromoji_part_of_speech\": {\n",
    "          \"type\": \"kuromoji_part_of_speech\",\n",
    "          \"stoptags\": [\"感動詞,フィラー\",\"接頭辞\",\"代名詞\",\"副詞\",\"助詞\",\"助動詞\",\"動詞,一般,*,*,*,終止形-一般\",\"名詞,普通名詞,副詞可能\"]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "# インデックス名を指定\n",
    "index_name = \"jsquad-knn\"\n",
    "\n",
    "try:\n",
    "    # 既に同名のインデックスが存在する場合、いったん削除を行う\n",
    "    print(\"# delete index\")\n",
    "    response = opensearch_client.indices.delete(index=index_name)\n",
    "    print(json.dumps(response, indent=2))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# インデックスを作成\n",
    "response = opensearch_client.indices.create(index_name, body=payload)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987187d0",
   "metadata": {},
   "source": [
    "### サンプルデータの格納\n",
    "\n",
    "サンプルデータにベクトルデータを追加し、OpenSearch に格納します。\n",
    "\n",
    "#### ベクトルデータを生成(埋め込み)\n",
    "\n",
    "DataFrame 形式に加工したサンプルデータの question フィールドと context フィールドを対象に、Bedrock 上で稼働している埋め込みモデルの推論エンドポイントを呼び出してベクトルデータを生成、結合する処理を実行します。\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> \n",
    "Bedrock での Batch 実行は、S3 経由であり、メモリ中のデータを invoke_model で処理する場合は、１件ずつ処理されるので、15分前後かかります。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693e23ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def get_df_with_embeddings(input_df, field_mappings, model_id, bedrock_region, batch_size):\n",
    "    output_df = pd.DataFrame([]) #create empty dataframe\n",
    "    df_list = np.array_split(input_df, input_df.shape[0]/batch_size)\n",
    "    for df in tqdm(df_list):\n",
    "        index = df.index #backup index number\n",
    "        df_with_embeddings = df\n",
    "        for field_mapping in field_mappings:\n",
    "            input_field_name = field_mapping[\"InputFieldName\"]\n",
    "            embedding_field_name = field_mapping[\"EmbeddingFieldName\"]\n",
    "            payload = {\n",
    "                \"inputText\": df_with_embeddings[input_field_name].values.tolist()[0]\n",
    "            }\n",
    "            body = json.dumps(payload)\n",
    "            bedrock_runtime_client = boto3.client(\"bedrock-runtime\", region_name=bedrock_region)\n",
    "            response = bedrock_runtime_client.invoke_model(modelId=model_id, body=body)\n",
    "            model_response = json.loads(response[\"body\"].read())\n",
    "            embeddings = [model_response[\"embedding\"]]\n",
    "\n",
    "            df_with_embeddings = pd.concat([df_with_embeddings.reset_index(drop=True), pd.Series(embeddings,name=embedding_field_name).reset_index(drop=True)],axis=1) #join embedding results to source dataframe\n",
    "            df_with_embeddings = df_with_embeddings.set_index(index) #restore index number\n",
    "\n",
    "        output_df = pd.concat([output_df, df_with_embeddings])\n",
    "    return output_df\n",
    "\n",
    "valid_df_with_embeddings = get_df_with_embeddings(\n",
    "    input_df=valid_df,\n",
    "    field_mappings=[\n",
    "        {\"InputFieldName\": \"question\", \"EmbeddingFieldName\": \"question_embedding\"},\n",
    "        {\"InputFieldName\": \"context\", \"EmbeddingFieldName\": \"context_embedding\"},\n",
    "    ],\n",
    "    model_id=model_id,\n",
    "    bedrock_region=default_region,\n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754ab7d7",
   "metadata": {},
   "source": [
    "実行後の DataFrame は以下の通りです。数値配列の question_embedding フィールドおよび context_embedding フィールドが追加されていることが確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e186c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df_with_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d968e8",
   "metadata": {},
   "source": [
    "#### ドキュメントのロード\n",
    "\n",
    "ドキュメントのロードを行います。ドキュメントのロードは \"OpenSearch の基本概念・基本操作の理解\" でも解説した通り bulk API を使用することで効率よく進められますが、データ処理フレームワークを利用することでより簡単にデータを取り込むことも可能です。本ワークショップでは、[AWS SDK for Pandas][aws-sdk-pandas] を使用したデータ取り込みを行います。\n",
    "\n",
    "[aws-sdk-pandas]: https://github.com/aws/aws-sdk-pandas\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> \n",
    "AOSS のベクトル検索コレクションの場合、ID 指定できません。<br>\n",
    "指定すると、`Document ID is not supported in create/index operation request` となります。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06a8b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "index_name = \"jsquad-knn\"\n",
    "response = wr.opensearch.index_df(\n",
    "    client=opensearch_client,\n",
    "    df=valid_df_with_embeddings,\n",
    "    use_threads=True,\n",
    "    index=index_name,\n",
    "    bulk_size=200, # 200 件ずつ書き込み\n",
    "    refresh=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e5eacc",
   "metadata": {},
   "source": [
    "response[\"success\"] の値が DataFrame の件数と一致しているかを確認します。<br>\n",
    "True が表示される場合は全件登録に成功していると判断できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4e7f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response[\"success\"] == valid_df[\"id\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0399b5",
   "metadata": {},
   "source": [
    "## 検索結果の比較\n",
    "\n",
    "テキスト検索とベクトル検索を実行し、結果を比較していきます。\n",
    "\n",
    "### テキスト検索\n",
    "\n",
    "テキスト検索のヒット率は検索キーワードとインデックスに格納されたコンテンツの内容、およびアナライザーによる正規化設定により左右されます。<br>\n",
    "テキスト検索では極力不要なキーワードは排除して検索が実行されることが好まれます。以下のような単語の組み合わせによる検索で性能を発揮します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7346435b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"jsquad-knn\"\n",
    "query = \"日本 梅雨 ない どこ\"\n",
    "\n",
    "payload = {\n",
    "  \"query\": {\n",
    "    \"match\": {\n",
    "      \"question\": {\n",
    "        \"query\": query,\n",
    "        \"operator\": \"and\"\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"_source\": False,\n",
    "  \"fields\": [\"question\", \"answers\", \"context\"],\n",
    "  \"size\": 10\n",
    "}\n",
    "response = opensearch_client.search(\n",
    "    index=index_name,\n",
    "    body=payload\n",
    ")\n",
    "pd.json_normalize(response[\"hits\"][\"hits\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bec157",
   "metadata": {},
   "source": [
    "一方、以下のような会話に近いクエリは、ノイズが増加するためうまく処理できない場合があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c135300c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"jsquad-knn\"\n",
    "query = \"日本で梅雨がない場所は？\"\n",
    "\n",
    "payload = {\n",
    "  \"query\": {\n",
    "    \"match\": {\n",
    "      \"question\": {\n",
    "        \"query\": query,\n",
    "        \"operator\": \"and\"\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"_source\": False,\n",
    "  \"fields\": [\"question\", \"answers\", \"context\"],\n",
    "  \"size\": 10\n",
    "}\n",
    "response = opensearch_client.search(\n",
    "    index=index_name,\n",
    "    body=payload\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dc612b",
   "metadata": {},
   "source": [
    "従来は、minimum_should_match といった[パラメーター][parameters]によるチューニングを行ってきました。<br>\n",
    "以下は検索クエリに含まれるトークンのうち 75% がマッチするドキュメントを返却するクエリです。\n",
    "\n",
    "[parameters]: https://opensearch.org/docs/latest/query-dsl/full-text/match/#parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c79e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"jsquad-knn\"\n",
    "query = \"日本で梅雨がない場所は？\"\n",
    "\n",
    "payload = {\n",
    "  \"query\": {\n",
    "    \"match\": {\n",
    "      \"question\": {\n",
    "        \"query\": query,\n",
    "        \"operator\": \"or\",\n",
    "        \"minimum_should_match\": \"75%\"\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"_source\": False,\n",
    "  \"fields\": [\"question\", \"answers\", \"context\"],\n",
    "  \"size\": 10\n",
    "}\n",
    "response = opensearch_client.search(\n",
    "    index=index_name,\n",
    "    body=payload\n",
    ")\n",
    "pd.json_normalize(response[\"hits\"][\"hits\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d21900",
   "metadata": {},
   "source": [
    "### ベクトル検索\n",
    "\n",
    "テキスト検索では対応が難しい会話に近い問い合わせ分をベクトル検索で処理していきます。<br>\n",
    "OpenSearch では [knn クエリ](https://opensearch.org/docs/latest/search-plugins/knn/approximate-knn/#get-started-with-approximate-k-nn)を使用してベクトル検索を実行します。vector フィールドにはベクトルデータを、k には取得したい近似ベクトルの件数を指定しています。\n",
    "OpenSearch は knn クエリも分散実行されるため、インデックスの構成によっては k の値と戻りの総件数の値が異なる場合があります。k の値と size の値はそろえることを推奨しています。詳細は [The number of returned results](https://opensearch.org/docs/latest/search-plugins/knn/approximate-knn/#the-number-of-returned-results) を参照してください。\n",
    "\n",
    "以下のコードでは、クエリテキストを Amazon Bedrock の推論エンドポイントに渡してベクトルデータを生成し、knn クエリの vector パラメーターに渡しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca49845",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"jsquad-knn\"\n",
    "model_id = \"amazon.titan-embed-text-v2:0\"\n",
    "query = \"日本で梅雨がない場所は？\"\n",
    "\n",
    "def text_to_embedding(text, region_name, model_id):\n",
    "    payload = {\n",
    "        \"inputText\": text\n",
    "    }\n",
    "    body = json.dumps(payload)\n",
    "    bedrock_runtime_client = boto3.client(\"bedrock-runtime\", region_name)\n",
    "    response = bedrock_runtime_client.invoke_model(modelId = model_id, body=body)\n",
    "    model_response = json.loads(response[\"body\"].read())\n",
    "    return model_response[\"embedding\"]\n",
    "\n",
    "vector = text_to_embedding(text=query, region_name=default_region, model_id=model_id)\n",
    "k = 10\n",
    "\n",
    "payload = {\n",
    "  \"query\": {\n",
    "    \"knn\": {\n",
    "      \"question_embedding\": {\n",
    "        \"vector\": vector,\n",
    "        \"k\": k\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"_source\": False,\n",
    "  \"fields\": [\"question\", \"answers\", \"context\"],\n",
    "  \"size\": k\n",
    "}\n",
    "response = opensearch_client.search(\n",
    "    index=index_name,\n",
    "    body=payload\n",
    ")\n",
    "pd.json_normalize(response[\"hits\"][\"hits\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04c0ce4",
   "metadata": {},
   "source": [
    "## ベクトル検索のスコアリング\n",
    "\n",
    "ベクトル検索は _score が 1 未満となります。ベクトル検索においては、クエリと対象ドキュメントの距離が近いほど距離の値は小さくなります。<br>\n",
    "したがって、距離を 0 から 1 の間で正規化したうえで、1 から距離を引いた値をスコア(関連度)としています。\n",
    "\n",
    "knn search では、上位 k 個のベクトルという条件以外に、こうした距離やスコアを使った絞り込みが可能です。フィルタリングには以下のオプションを使用可能です。これらのオプションは k と併用不可能です。\n",
    "\n",
    "- min_score\n",
    "- max_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20466c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"jsquad-knn\"\n",
    "query = \"日本で梅雨がない場所は？\"\n",
    "\n",
    "def text_to_embedding(text, region_name, model_id):\n",
    "    payload = {\n",
    "        \"inputText\": text\n",
    "    }\n",
    "    body = json.dumps(payload)\n",
    "    bedrock_runtime_client = boto3.client(\"bedrock-runtime\", region_name)\n",
    "    response = bedrock_runtime_client.invoke_model(modelId = model_id, body=body)\n",
    "    model_response = json.loads(response[\"body\"].read())\n",
    "    return model_response[\"embedding\"]\n",
    "\n",
    "vector = text_to_embedding(text=query, region_name=default_region, model_id=model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83746a33",
   "metadata": {},
   "source": [
    "min_score に 0.7 をセットした結果は以下の通りです。<br>\n",
    "0.7 以上のスコアのベクトルのみが返却されました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a79225",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "\n",
    "payload = {\n",
    "  \"query\": {\n",
    "    \"knn\": {\n",
    "      \"question_embedding\": {\n",
    "        \"vector\": vector,\n",
    "        \"min_score\": 0.7\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"_source\": False,\n",
    "  \"fields\": [\"question\", \"answers\", \"context\"],\n",
    "  \"size\": k,\n",
    "}\n",
    "response = opensearch_client.search(\n",
    "    index=index_name,\n",
    "    body=payload\n",
    ")\n",
    "pd.json_normalize(response[\"hits\"][\"hits\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc658d8",
   "metadata": {},
   "source": [
    "max_distance に 0.3 をセットしても同様の結果が得られます。<br>\n",
    "`1 - 0.3 = 0.7` に相当するスコアのベクトルが変えるためです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd8bf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "\n",
    "payload = {\n",
    "  \"query\": {\n",
    "    \"knn\": {\n",
    "      \"question_embedding\": {\n",
    "        \"vector\": vector,\n",
    "        \"max_distance\": 0.3\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"_source\": False,\n",
    "  \"fields\": [\"question\", \"answers\", \"context\"],\n",
    "  \"size\": k, \n",
    "}\n",
    "response = opensearch_client.search(\n",
    "    index=index_name,\n",
    "    body=payload\n",
    ")\n",
    "pd.json_normalize(response[\"hits\"][\"hits\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aa6ff0",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "ラボを通して、全文検索では対応が難しいクエリをベクトル検索で処理できることが確認できました。<br>\n",
    "時間がある方は、続いて以下のラボも実施してみましょう。\n",
    "\n",
    "- [ニューラル検索の実装 (Amazon Bedrock 編)](#neural-search-with-sagemaker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2be33dc",
   "metadata": {},
   "source": [
    "## 後片付け\n",
    "\n",
    "### データセット削除\n",
    "ダウンロードしたデータセットを削除します。<br>\n",
    "./dataset ディレクトリ配下に何もない場合は、./dataset ディレクトリも合わせて削除します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e59e4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm -rf {dataset_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360c2dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%rmdir ./dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1ff69b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# ニューラル検索の実装 (Amazon Bedrock 編)<a id=\"neural-search-with-sagemaker\"></a>\n",
    "\n",
    "> この章は、`neural-search-with-sagemaker.ipynb` を元に作成しています。\n",
    "\n",
    "## 概要\n",
    "\n",
    "本ラボでは、テキストクエリを内部的にベクトルに変換してベクトル検索を行うニューラル検索を実装していきます。<br>\n",
    "ベクトルの生成は、Amazon SageMaker 上にデプロイした密ベクトル埋め込みモデルを利用します。\n",
    "\n",
    "### 前提事項\n",
    "\n",
    "本ラボは、[ベクトル検索の実装 (Amazon Bedrock 編)](#vector-search-with-sagemaker) を完了していることを前提として作られています。<br>\n",
    "ベクトル検索の基本的な要素の解説や、ML モデルのデプロイなど、本ラボを実施するにあたって必要な作業も含まれているため、事前に上記のラボを完了させてください。\n",
    "\n",
    "### ニューラル検索について\n",
    "\n",
    "ニューラル検索は、OpenSearch に入力されたテキストや画像のクエリデータを、OpenSearch 側でベクトルに変換し、登録と検索を実行する機能です。<br>\n",
    "一般的なベクトル検索は、クライアント側で用意したベクトルを使用したデータ登録や検索を行う必要があります。従来のテキスト検索に加えてベクトル検索を実装しようとする場合、バックエンド側に埋め込みモデルを呼び出す処理を実装する必要があります。\n",
    "\n",
    "<img src=\"./img/dense-vector-embedding-with-backend.png\" width=\"1024\">\n",
    "\n",
    "ニューラル検索では、OpenSearch がバックエンドの責務も担います。OpenSearch はユーザークエリをもとに埋め込みモデルを呼び出し、ベクトルの生成を行います。生成したベクトルは、そのまま格納、ないしはベクトル検索に使用します。<br>\n",
    "ニューラル検索を活用することで、クライアント側の改修を最小限に抑えつつベクトル検索を導入可能となります。\n",
    "\n",
    "<img src=\"./img/dense-vector-embedding-with-connector.png\" width=\"1024\">\n",
    "\n",
    "ニューラル検索は以下のコンポーネントで構成されています\n",
    "\n",
    "- モデル([リモートモデル][remote-models]): 外部サービス上にホストされた ML モデル。接続を行うためには、コネクターが必要となる。\n",
    "- [コネクター][connector]: Amazon SageMaker の推論エンドポイントなど、外部エンドポイントへの接続情報を管理するコンポーネント\n",
    "- Embedding processor: パイプラインから与えられたデータを ML モデルに渡すためのプロセッサ。テキスト埋め込み用の [Text embedding processor][text-embedding]、テキスト+画像のマルチモーダル埋め込み用の [Text/image embedding processor][text-image-embedding] など、元のデータフォーマットによって異なるプロセッサが存在する。\n",
    "- [Ingest pipelines][ingest-pipelines]: ドキュメント登録時に加工処理を行うパイプライン。1 つ以上のプロセッサで構成されている。Embedding processor を呼び出すことで、ドキュメントのテキストもしくは画像データが格納されたバイナリフィールドからベクトル埋め込みを生成し、元のテキストとベクトルの両方をk-NNインデックスに保存することが可能。\n",
    "- [Search pipelines][search-pipelines]: 検索クエリもしくは検索結果の加工処理を行うパイプライン。1 つ以上のプロセッサで構成されている。Embedding processor を呼び出すことで、クエリからベクトル埋め込みを生成し、ベクトルフィールドに対する検索を実行することが可能となる。\n",
    "\n",
    "### コネクター\n",
    "\n",
    "コネクターは、外部サービスとの連携の大部分を担っています。サービスのエンドポイントや、OpenSearch から渡されたデータを外部サービス向けのリクエストペイロードに書き換えるための定義、外部サービスから受け取った応答を OpenSearch 向けのフォーマットに書き換えるための定義といった情報を保持しています。\n",
    "\n",
    "### ラボの構成\n",
    "\n",
    "本ラボでは、ノートブック環境（EC2 へ Remote Develop 接続した VSCode）および Amazon OpenSearch Serverless、Amazon Bedrock を使用します\n",
    "\n",
    "<img src=\"./img/architecture-with-bedrock.png\" width=\"512\">\n",
    "\n",
    "### 使用するデータセット\n",
    "\n",
    "本ラボでは、[ベクトル検索の実装 (Amazon Bedrock 編)](#vector-search-with-sagemaker) と同様に、[JGLUE][jglue] 内の FAQ データセットである [JSQuAD][jsquad] を使用します。\n",
    "\n",
    "[remote-models]: https://opensearch.org/docs/latest/ml-commons-plugin/remote-models/index/\n",
    "[connector]: https://opensearch.org/docs/latest/ml-commons-plugin/remote-models/connectors/\n",
    "[text-embedding]: https://opensearch.org/docs/latest/ingest-pipelines/processors/text-embedding/\n",
    "[text-image-embedding]: https://opensearch.org/docs/latest/ingest-pipelines/processors/text-image-embedding/\n",
    "[ingest-pipelines]: https://opensearch.org/docs/latest/ingest-pipelines/\n",
    "[search-pipelines]: https://opensearch.org/docs/latest/search-plugins/search-pipelines/index/\n",
    "[jglue]: https://github.com/yahoojapan/JGLUE/tree/main\n",
    "[jsquad]: https://github.com/yahoojapan/JGLUE/tree/main/datasets/jsquad-v1.3\n",
    "[bge-m3]: https://huggingface.co/BAAI/bge-m3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e789fc08",
   "metadata": {},
   "source": [
    "## 事前作業\n",
    "ベクトル検索の実装で実施済みです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae7f6f2",
   "metadata": {},
   "source": [
    "## リソース準備\n",
    "\n",
    "ベクトル検索の実装で実施済みです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d961c7f6",
   "metadata": {},
   "source": [
    "### OpenSearch 関連リソースの作成\n",
    "\n",
    "#### インデックスの作成\n",
    "id、question、context、answers フィールドを格納するための文字列型フィールドに加えて、question、context フィールドから生成したベクトルデータを格納するための context_dense_embedding、question_sparse_embedding フィールドを持つインデックスを作成します。\n",
    "\n",
    "文字列型フィールドについては、テキスト検索でもある程度の検索精度を出せるように、id フィールドを除いて kuromoji のカスタムアナライザーをセットしています。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291f75b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"id\": {\"type\": \"keyword\"},\n",
    "      \"question\": {\"type\": \"text\", \"analyzer\": \"custom_kuromoji_analyzer\"},\n",
    "      \"context\":  {\"type\": \"text\", \"analyzer\": \"custom_kuromoji_analyzer\"},\n",
    "      \"answers\":  {\"type\": \"text\", \"analyzer\": \"custom_kuromoji_analyzer\"},\n",
    "      \"question_embedding\": {\n",
    "        \"type\": \"knn_vector\",\n",
    "        \"dimension\": 1024,\n",
    "        \"space_type\": \"l2\",\n",
    "        \"method\": {\n",
    "          \"name\": \"hnsw\",\n",
    "          \"engine\": \"faiss\",\n",
    "        }\n",
    "      },\n",
    "      \"context_embedding\": {\n",
    "        \"type\": \"knn_vector\",\n",
    "        \"dimension\": 1024,\n",
    "        \"space_type\": \"l2\",\n",
    "        \"method\": {\n",
    "          \"name\": \"hnsw\",\n",
    "          \"engine\": \"faiss\",\n",
    "        },\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"settings\": {\n",
    "    \"index.knn\": True,\n",
    "    \"analysis\": {\n",
    "      \"analyzer\": {\n",
    "        \"custom_kuromoji_analyzer\": {\n",
    "          \"char_filter\": [\"icu_normalizer\"],\n",
    "          \"filter\": [\n",
    "              \"kuromoji_baseform\",\n",
    "              \"custom_kuromoji_part_of_speech\"\n",
    "          ],\n",
    "          \"tokenizer\": \"kuromoji_tokenizer\",\n",
    "          \"type\": \"custom\"\n",
    "        }\n",
    "      },\n",
    "      \"filter\": {\n",
    "        \"custom_kuromoji_part_of_speech\": {\n",
    "          \"type\": \"kuromoji_part_of_speech\",\n",
    "          \"stoptags\": [\"感動詞,フィラー\",\"接頭辞\",\"代名詞\",\"副詞\",\"助詞\",\"助動詞\",\"動詞,一般,*,*,*,終止形-一般\",\"名詞,普通名詞,副詞可能\"]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "# インデックス名を指定\n",
    "index_name = \"jsquad-neural-search\"\n",
    "\n",
    "try:\n",
    "    # 既に同名のインデックスが存在する場合、いったん削除を行う\n",
    "    print(\"# delete index\")\n",
    "    response = opensearch_client.indices.delete(index=index_name)\n",
    "    print(json.dumps(response, indent=2))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# インデックスを作成\n",
    "response = opensearch_client.indices.create(index_name, body=payload)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fe74a4",
   "metadata": {},
   "source": [
    "#### OpenSearch へのモデル登録\n",
    "\n",
    "Bedrock 上のモデルを呼び出すためのコンポーネントを作成します。<br>\n",
    "モデルは、コネクタと呼ばれる外部接続を定義したコンポーネントで構成されています。<br>\n",
    "今回の構成では、モデルは Text Embedding Processor と呼ばれる、入力テキストをベクトルに変換するためのプロセッサーから呼び出されます。\n",
    "\n",
    "##### コネクタ用 IAM Role ARN の確認\n",
    "\n",
    "OpenSearch コネクタから AWS サービスに接続する際、任意の IAM ロールの権限を引き受ける必要があります。<br>\n",
    "引受対象の IAM ロールを CloudFormation スタックの出力から取得します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f28dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "opensearch_connector_role_arn = os.getenv(\"AOSS_ROLE_ARN\")\n",
    "opensearch_connector_role_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bcadd2",
   "metadata": {},
   "source": [
    "##### コネクタの作成\n",
    "\n",
    "Amazon Bedrock 上のモデルを呼び出す定義を記載したコネクタを作成します。<br>\n",
    "コネクタは、OpenSearch におけるモデルの一要素です。\n",
    "\n",
    "コネクタの処理の流れは以下の通りです。\n",
    "\n",
    "1. pre_process_function の定義を元に、OpenSearch の Ingestion pipeline もしくは Search pipline 内の Text embeddding processor から与えられた入力から、推論エンドポイントに与えるパラメーターを作成\n",
    "1. pre_process_function によって変換されたパラメーターを元に、request_body の定義に沿ってペイロードを組み立て、推論エンドポイントの呼び出しを行う\n",
    "1. post_process_function の定義を元に、推論エンドポイントから返却された推論結果を加工し、Text embedding processor に返却"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca2e73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_name = \"amazon.titan-embed-text-v2:0\"\n",
    "\n",
    "payload = {\n",
    "  \"name\": embedding_model_name, \n",
    "  \"description\": \"Remote connector for \" + embedding_model_name,\n",
    "  \"version\": 1, \n",
    "  \"protocol\": \"aws_sigv4\",\n",
    "  \"credential\": {\n",
    "    \"roleArn\": opensearch_connector_role_arn\n",
    "  },\n",
    "  \"parameters\": {\n",
    "    \"region\": default_region,\n",
    "    \"service_name\": \"bedrock\",\n",
    "    \"model\": embedding_model_name,\n",
    "    \"dimensions\": 1024,\n",
    "    \"normalize\": True,\n",
    "    \"embeddingTypes\": [\"float\"],    \n",
    "  },\n",
    "  \"actions\": [\n",
    "    {\n",
    "      \"action_type\": \"predict\",\n",
    "      \"method\": \"POST\",\n",
    "      \"headers\": {\n",
    "          \"content-type\": \"application/json\",\n",
    "          \"x-amz-content-sha256\": \"required\",\n",
    "      },\n",
    "      \"url\": \"https://bedrock-runtime.${parameters.region}.amazonaws.com/model/${parameters.model}/invoke\",\n",
    "      \"pre_process_function\": \"connector.pre_process.bedrock.embedding\",\n",
    "      \"request_body\": '{ \"inputText\": \"${parameters.inputText}\", \"dimensions\": ${parameters.dimensions}, \"normalize\": ${parameters.normalize}, \"embeddingTypes\": ${parameters.embeddingTypes} }',\n",
    "      \"post_process_function\": \"connector.post_process.bedrock.embedding\",\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "# API の実行\n",
    "response = opensearch_client.http.post(\"/_plugins/_ml/connectors/_create\", body=payload)\n",
    "\n",
    "# 結果からコネクタ ID を取得\n",
    "opensearch_embedding_connector_id = response[\"connector_id\"]\n",
    "print(\"embedding connector id: \" + opensearch_embedding_connector_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86f2b75",
   "metadata": {},
   "source": [
    "##### OpenSearch へのモデル登録\n",
    "\n",
    "コネクタを元に、OpenSearch にモデル情報を登録します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b405ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"name\": embedding_model_name,\n",
    "    \"description\": embedding_model_name,\n",
    "    \"function_name\": \"remote\",\n",
    "    \"connector_id\": opensearch_embedding_connector_id\n",
    "}\n",
    "response = opensearch_client.http.post(\"/_plugins/_ml/models/_register?deploy=true\", body=payload)\n",
    "\n",
    "opensearch_embedding_model_id = response['model_id']\n",
    "\n",
    "for i in range(300):\n",
    "    ml_model_status = opensearch_client.http.get(\"/_plugins/_ml/models/\"+ opensearch_embedding_model_id)\n",
    "    model_state = ml_model_status.get(\"model_state\")\n",
    "    if model_state in [\"DEPLOYED\", \"PARTIALLY_DEPLOYED\"]:\n",
    "        break\n",
    "    time.sleep(1)\n",
    "\n",
    "if model_state == \"DEPLOYED\":\n",
    "    print(\"embedding model \" + opensearch_embedding_model_id + \" is deployed successfully\")\n",
    "elif model_state == \"PARTIALLY_DEPLOYED\":\n",
    "    print(\"embedding model \" + opensearch_embedding_model_id + \" is deployed only partially\")\n",
    "else:\n",
    "    raise Exception(\"embedding model \" + opensearch_embedding_model_id + \" deployment failed\")\n",
    "\n",
    "print(ml_model_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f19a0c",
   "metadata": {},
   "source": [
    "#### モデルの呼び出しテスト\n",
    "\n",
    "OpenSearch 経由で Amazon Bedrock 上の埋め込みモデルを実行できることを確認します。<br>\n",
    "モデルの呼び出し方は 2 パターンあります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e813d31",
   "metadata": {},
   "source": [
    "##### Text embedding processor からの呼び出しを想定したテストパターン\n",
    "\n",
    "Text embedding processor からの呼び出しを想定する場合は、以下パスの API を使用します。<br>\n",
    "Text embedding processor が text_embedding モデルを呼び出す際のパラメーターキーは text_docs で固定されています。同パラメーターには、クライアントからの入力テキストがセットされています。\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> \n",
    "AOSS では、text_embedding 経由の呼び出しはサポートされていません。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586730cf",
   "metadata": {},
   "source": [
    "##### pre_process_function をバイパスするパターン\n",
    "\n",
    "ML モデル配下の predict API を直接呼び出してテストを行うことも可能です。<br>\n",
    "この場合 pre_process_function は呼び出されず、parameters に記載した値が直接コネクタで指定した推論エンドポイントに渡されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930282a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/_plugins/_ml/models/\" + opensearch_embedding_model_id + \"/_predict\"\n",
    "payload = {\n",
    "  \"parameters\": {\n",
    "    \"inputText\": \"日本で梅雨がないのはどこ？\"\n",
    "  }\n",
    "}\n",
    "response = opensearch_client.http.post(path, body=payload)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e54397",
   "metadata": {},
   "source": [
    "#### Ingestion pipeline の作成\n",
    "\n",
    "データ登録時にベクトル埋め込みを行う Ingestion pipeline を作成します。埋め込み元のデータはテキストであるため、今回は [Text embedding processor](https://opensearch.org/docs/latest/ingest-pipelines/processors/text-embedding/) を使用します。<br>\n",
    "Text embedding processor では、埋め込みの元となるフィールドと埋め込みを格納するフィールドのマッピングを field_map 内で定義し、model_id には埋め込みに用いるモデル ID を指定します。\n",
    "\n",
    "以下は Ingestion pipeline による埋め込みのイメージです\n",
    "\n",
    "<img src=\"./img/neural-search-ingestion.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc63229",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "  \"processors\": [\n",
    "    {\n",
    "      \"text_embedding\": {\n",
    "        \"model_id\": opensearch_embedding_model_id,\n",
    "        \"field_map\": {\n",
    "            \"question\": \"question_embedding\",\n",
    "            \"context\": \"context_embedding\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "ingestion_pipeline_id = f\"{embedding_model_name}_neural_search_ingestion\"\n",
    "\n",
    "response = opensearch_client.http.put(\"/_ingest/pipeline/\" + ingestion_pipeline_id, body=payload)\n",
    "print(response)\n",
    "\n",
    "response = opensearch_client.http.get(\"/_ingest/pipeline/\" + ingestion_pipeline_id)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90458c3",
   "metadata": {},
   "source": [
    "作成したパイプラインは _simulate API でテストが可能です。<br>\n",
    "context_embedding および question_embedding フィールドが含まれていれば正常にパイプラインが動作していると判断できます。\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> \n",
    "AOSS では、pipeline_id 指定の _simulate API は、使用できませんので、厳密には「作成した」パイプラインのテストはできません。<br>\n",
    "ただし、以下のように、同じ内容を再定義してテストすることは可能です。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4235aefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "payload = {\n",
    "  \"docs\": [\n",
    "    {\n",
    "      \"_index\": \"testindex1\",\n",
    "      \"_id\": \"1\",\n",
    "      \"_source\":{\n",
    "         \"question\": \"日本で梅雨がないのはどこか。\",\n",
    "         \"context\": \"梅雨 [SEP] 梅雨（つゆ、ばいう）は、北海道と小笠原諸島を除く日本、朝鮮半島南部、中国の南部から長江流域にかけての沿海部、および台湾など、東アジアの広範囲においてみられる特有の気象現象で、5月から7月にかけて来る曇りや雨の多い期間のこと。雨季の一種である。 \",\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    "  \"pipeline\": {\n",
    "    'processors': [\n",
    "      {\n",
    "        'text_embedding': {\n",
    "          'model_id': opensearch_embedding_model_id, \n",
    "          'field_map': {\n",
    "            'question': 'question_embedding', \n",
    "            'context': 'context_embedding'\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "response = opensearch_client.http.post(\"/_ingest/pipeline/_simulate\", body=payload)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4e2511",
   "metadata": {},
   "source": [
    "#### Search pipeline の作成\n",
    "\n",
    "クライアントから入力されたテキストベースのクエリをベクトルベースのクエリに変換するための Search pipeline を作成します。<br>\n",
    "Search pipeline は、検索時のクエリ書き換え用の Request processors、レスポンス書き換え用の Response processors、スコアなどの検索結果を書き換える Search phase results processors の 3 タイプが存在します。\n",
    "\n",
    "<img src='./img/search-pipelines.png'>\n",
    "\n",
    "今回使用する [Neural query enricher processor][neural-query-enricher] は Request processors に属しています。このプロセッサは、後述する Neural query を実行する際のデフォルトモデルをセットするものです。\n",
    "\n",
    "[search-request-processors]: https://opensearch.org/docs/latest/search-plugins/search-pipelines/search-processors/#search-request-processors\n",
    "[search-response-processors]: https://opensearch.org/docs/latest/search-plugins/search-pipelines/search-processors/#search-response-processors\n",
    "[search-phase-results-processors]: https://opensearch.org/docs/latest/search-plugins/search-pipelines/search-processors/#search-phase-results-processors\n",
    "[neural-query-enricher]: https://opensearch.org/docs/latest/search-plugins/search-pipelines/neural-query-enricher/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9c34a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload={\n",
    "  \"request_processors\": [\n",
    "    {\n",
    "      \"neural_query_enricher\" : {\n",
    "        \"default_model_id\": opensearch_embedding_model_id\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "# パイプライン ID の指定\n",
    "search_pipeline_id = f\"{embedding_model_name}_neural_search_query\"\n",
    "# パイプライン作成 API の呼び出し\n",
    "response = opensearch_client.http.put(\"/_search/pipeline/\" + search_pipeline_id, body=payload)\n",
    "print(response)\n",
    "\n",
    "response = opensearch_client.http.get(\"/_search/pipeline/\" + search_pipeline_id)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50f2bd4",
   "metadata": {},
   "source": [
    "Search pipeline についてはテスト用の API が提供されていないため、実際に Neural search を実行して動作を確認していきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fde0ed8",
   "metadata": {},
   "source": [
    "## ニューラル検索の実行\n",
    "\n",
    "データセットを OpenSearch にロードし、検索を実行していきます。\n",
    "\n",
    "### データロード\n",
    "\n",
    "DataFrame 形式に変換したサンプルデータセットを OpenSearch に登録していきます。DataFrame にはベクトルデータは含まれていませんが、Ingestion pipeline を通じてデータを登録することでベクトルデータが OpenSearch 側で生成・登録されます。\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> \n",
    "AOSS のベクトル検索コレクションでは、Document ID 指定ができません。<br>\n",
    "指定すると、`Document ID is not supported in create/index operation request` となります。\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> \n",
    "AOSS 側ではなく、Bedrock 側の問題かもしれませんが、シングルスレッドの逐次処置にしないと最後まで登録が完了しません。このため、以下の処理は、15分前後かかります。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44c04d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "index_name = \"jsquad-neural-search\"\n",
    "response = wr.opensearch.index_df(\n",
    "    client=opensearch_client,\n",
    "    df=valid_df,\n",
    "    use_threads=False,\n",
    "    index=index_name,\n",
    "    bulk_size=1, # 1 件ずつ書き込み\n",
    "    refresh=False,\n",
    "    pipeline=ingestion_pipeline_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a934a14f",
   "metadata": {},
   "source": [
    "response[\"success\"] の値が DataFrame の件数と一致しているかを確認します。<br>\n",
    "`np.True` が表示される場合は全件登録に成功していると判断できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08ba6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response[\"success\"] == valid_df[\"id\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd347a7f",
   "metadata": {},
   "source": [
    "パイプラインを通して登録されたドキュメントにベクトルデータが登録されていることを確認します。<br>\n",
    "**_source.question_embedding** および **_source.context_embedding** フィールドに数値配列が格納されていれば、パイプラインによる埋め込み生成とベクトルデータの格納が正常に行われたと判断することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b3b646",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "index_name = \"jsquad-neural-search\"\n",
    "payload = {\n",
    "  \"size\": 10,\n",
    "  \"query\": {\n",
    "    \"match_all\": {}\n",
    "  },\n",
    "}\n",
    "# 検索 API を実行\n",
    "response = opensearch_client.search(\n",
    "    body = payload,\n",
    "    index = index_name,\n",
    "    filter_path = \"hits.hits\"\n",
    ")\n",
    "\n",
    "# 結果を表示\n",
    "pd.json_normalize(response[\"hits\"][\"hits\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850e67c5",
   "metadata": {},
   "source": [
    "### Neural query によるニューラル検索の実行\n",
    "\n",
    "[Neural query][neural] を使うことで、Search pipeline から埋め込みモデルを呼び出し、ユーザーのクエリデータを OpenSearch 側でベクトルデータに変換してから内部的にベクトル検索を実行することが可能となります。<br>\n",
    "仕組みは以下の通りです。model_id で指定した埋め込みモデルを使用し、query_text パラメーターに入力されたクエリテキストから生成したベクトルで knn query を実行しています。\n",
    "\n",
    "<img src=\"./img/neural-search-query.png\">\n",
    "\n",
    "以下は question フィールドから生成された question_dense_embedding フィールドに対する Neural query の実行サンプルです。\n",
    "\n",
    "[neural]: https://opensearch.org/docs/latest/query-dsl/specialized/neural/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68ea1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "index_name = \"jsquad-neural-search\"\n",
    "query = \"日本で梅雨がない場所は？\"\n",
    "payload = {\n",
    "  \"size\": 10,\n",
    "  \"query\": {\n",
    "    \"neural\": {\n",
    "      \"question_embedding\": {\n",
    "        \"query_text\": query, \n",
    "        \"model_id\": opensearch_embedding_model_id,\n",
    "        \"k\": 10\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"_source\" : False,\n",
    "  \"fields\": [\"question\", \"answers\",  \"context\"]\n",
    "}\n",
    "# 検索 API を実行\n",
    "response = opensearch_client.search(\n",
    "    body = payload,\n",
    "    index = index_name,\n",
    "    filter_path = \"hits.hits\"\n",
    ")\n",
    "\n",
    "# 結果を表示\n",
    "pd.json_normalize(response[\"hits\"][\"hits\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5706f23e",
   "metadata": {},
   "source": [
    "Neural query を使用することで、クライアントはテキストとモデル ID を渡すだけで、裏でベクトル検索が実行されるようになりました。\n",
    "\n",
    "ただ、クライアントがモデル ID を検索の都度指定するのは不便に思えます。\n",
    "\n",
    "そこで、先ほどの Search pipeline を使用して、クライアントがモデル ID を指定せずに Neural query を実行できるようにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675aa505",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "index_name = \"jsquad-neural-search\"\n",
    "query = \"日本で梅雨がない場所は？\"\n",
    "payload = {\n",
    "  \"size\": 10,\n",
    "  \"query\": {\n",
    "    \"neural\": {\n",
    "      \"question_embedding\": {\n",
    "        \"query_text\": query, \n",
    "        # model_id の指定は行わない\n",
    "        \"k\": 10\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"_source\" : False,\n",
    "  \"fields\": [\"question\", \"answers\",  \"context\"]\n",
    "}\n",
    "# 検索 API を実行\n",
    "response = opensearch_client.search(\n",
    "    body = payload,\n",
    "    index = index_name,\n",
    "    filter_path = \"hits.hits\",\n",
    "    search_pipeline = search_pipeline_id # 新たに追加\n",
    ")\n",
    "\n",
    "# 結果を表示\n",
    "pd.json_normalize(response[\"hits\"][\"hits\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2915c421",
   "metadata": {},
   "source": [
    "### 距離とスコアによるフィルタリング\n",
    "\n",
    "[ベクトル検索の実装 (Amazon Bedrock 編)](#vector-search-with-sagemaker) でも解説した、min_score および max_distance によるフィルタリングは Neural query でも利用可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b5a56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "index_name = \"jsquad-neural-search\"\n",
    "query = \"日本で梅雨がない場所は？\"\n",
    "payload = {\n",
    "  \"size\": 10,\n",
    "  \"query\": {\n",
    "    \"neural\": {\n",
    "      \"question_embedding\": {\n",
    "        \"query_text\": query, \n",
    "        \"min_score\": 0.7\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"_source\" : False,\n",
    "  \"fields\": [\"question\", \"answers\",  \"context\"]\n",
    "}\n",
    "# 検索 API を実行\n",
    "response = opensearch_client.search(\n",
    "    body = payload,\n",
    "    index = index_name,\n",
    "    filter_path = \"hits.hits\",\n",
    "    search_pipeline = search_pipeline_id\n",
    ")\n",
    "\n",
    "# 結果を表示\n",
    "pd.json_normalize(response[\"hits\"][\"hits\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd37e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "index_name = \"jsquad-neural-search\"\n",
    "query = \"日本で梅雨がない場所は？\"\n",
    "payload = {\n",
    "  \"size\": 10,\n",
    "  \"query\": {\n",
    "    \"neural\": {\n",
    "      \"question_embedding\": {\n",
    "        \"query_text\": query, \n",
    "        \"max_distance\": 0.3\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"_source\" : False,\n",
    "  \"fields\": [\"question\", \"answers\",  \"context\"]\n",
    "}\n",
    "# 検索 API を実行\n",
    "response = opensearch_client.search(\n",
    "    body = payload,\n",
    "    index = index_name,\n",
    "    filter_path = \"hits.hits\",\n",
    "    search_pipeline = search_pipeline_id\n",
    ")\n",
    "\n",
    "# 結果を表示\n",
    "pd.json_normalize(response[\"hits\"][\"hits\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0023c809",
   "metadata": {},
   "source": [
    "### Appendix: ニューラル検索と従来のベクトル検索を組み合わせて使用する\n",
    "\n",
    "本ラボでは、ニューラル検索を Ingest pipeline による登録データのテキスト(または画像)→ベクトル変換と、Search pipeline による検索クエリのベクトル変換を組み合わせて実装していきました。<br>\n",
    "Ingest pipeline と Search pipeline の併用は、実は必須ではありません。以下のようにどちらか一方のみを使用することもできます。\n",
    "\n",
    "- データ登録は Ingest pipeline を通じて行うが、検索ではバックエンド側でベクトル生成を行ったうえで通常の knn query を用いる\n",
    "- データ登録はバックエンド側でベクトル生成を行ったうえで、Ingest pipeline を使わず通常の bulk API で登録する。検索でのみ Search pipeline を介した neural query を用いる\n",
    "\n",
    "例えば、以下のようなユースケースが考えられます。\n",
    "\n",
    "- 初期構築時に極めて大量のベクトル登録を行う必要があるため、バッチ推論を使用して非同期でベクトルデータを生成し、通常の bulk API で登録を実施。初期構築後の部分更新、検索は Neural query で行う\n",
    "\n",
    "以下のサンプルコードでは、[ベクトル検索の実装 (Amazon Bedrock 編)](#vector-search-with-sagemaker) で作成した kNN 検索用のインデックスに対して Neural query を実行しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eac750",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "index_name = \"jsquad-knn\"\n",
    "query = \"日本で梅雨がない場所は？\"\n",
    "payload = {\n",
    "  \"size\": 10,\n",
    "  \"query\": {\n",
    "    \"neural\": {\n",
    "      \"question_embedding\": {\n",
    "        \"query_text\": query, \n",
    "        \"k\": 10\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"_source\" : False,\n",
    "  \"fields\": [\"question\", \"answers\",  \"context\"]\n",
    "}\n",
    "# 検索 API を実行\n",
    "response = opensearch_client.search(\n",
    "    body = payload,\n",
    "    index = index_name,\n",
    "    filter_path = \"hits.hits\",\n",
    "    search_pipeline = search_pipeline_id\n",
    ")\n",
    "\n",
    "# 結果を表示\n",
    "pd.json_normalize(response[\"hits\"][\"hits\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e927cf",
   "metadata": {},
   "source": [
    "また、本ラボで構築した Neural search 用のインデックスに対して、knn query を実行することもできます。以下はサンプルコードです。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74bedf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"jsquad-neural-search\"\n",
    "query = \"日本で梅雨がない場所は？\"\n",
    "\n",
    "def text_to_embedding(text, region_name, model_id):\n",
    "    payload = {\n",
    "        \"inputText\": text\n",
    "    }\n",
    "    body = json.dumps(payload)\n",
    "    bedrock_runtime_client = boto3.client(\"bedrock-runtime\", region_name)\n",
    "    response = bedrock_runtime_client.invoke_model(modelId = model_id, body=body)\n",
    "    model_response = json.loads(response[\"body\"].read())\n",
    "    return model_response[\"embedding\"]\n",
    "\n",
    "\n",
    "vector = text_to_embedding(text=query, region_name=default_region, model_id=model_id)\n",
    "k = 10\n",
    "\n",
    "payload = {\n",
    "  \"query\": {\n",
    "    \"knn\": {\n",
    "      \"question_embedding\": {\n",
    "        \"vector\": vector,\n",
    "        \"k\": k\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"_source\": False,\n",
    "  \"fields\": [\"question\", \"answers\", \"context\"],\n",
    "  \"size\": k\n",
    "}\n",
    "response = opensearch_client.search(\n",
    "    index=index_name,\n",
    "    body=payload\n",
    ")\n",
    "pd.json_normalize(response[\"hits\"][\"hits\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a98bb0",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "ラボを通して、OpenSearch 側でベクトル変換を行うニューラル検索の機能を確認できました。時間がある方は、続いて以下のラボも実施してみましょう。\n",
    "\n",
    "- ~~[スパース検索の実装 (Amazon SageMaker 編)](../sparse-search/sparse-search-with-sagemaker.ipynb)~~\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> \n",
    "東京リージョンの Bedrock では、スパース検索が可能なモデルは使用できないので、保留にしました。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41ce9a8",
   "metadata": {},
   "source": [
    "## 後片付け\n",
    "\n",
    "### データセット削除\n",
    "ダウンロードしたデータセットを削除します。./dataset ディレクトリ配下に何もない場合は、./dataset ディレクトリも合わせて削除します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e31c39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm -rf {dataset_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184dec96",
   "metadata": {},
   "outputs": [],
   "source": [
    "%rmdir ./dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ace9b0",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# ハイブリッド検索 (Amazon Bedrock 編)\n",
    "\n",
    "> この章は、`hybrid-search-with-sagemaker.ipynb` を元に作成しています。\n",
    "\n",
    "## 概要\n",
    "\n",
    "本ラボでは、テキスト検索、ベクトル検索を組み合わせたハイブリッド検索を実装していきます。\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> \n",
    "東京リージョンの Bedrock では、スパース検索が可能なモデルは使用できないので、スパース検索は除外しました。\n",
    "</div>\n",
    "\n",
    "### ハイブリッド検索の概要\n",
    "\n",
    "ハイブリッド検索は、複数の検索を実行し、各結果をマージ、スコアを平準化したうえでランク付けを行う機能です。<br>\n",
    "OpenSearch では [Hybrid query][hybrid-search] と [Normalization processor][normalization-processor] を組み合わせることでハイブリッド検索を実装することができます。<br>\n",
    "Hybrid query は複数のクエリを実行した結果を組み合わせるものです。単に Hybrid query を実行するだけでは、個々のクエリごとのスコア計算方法やベースのスコア値が大きく異なることで偏った結果となるため、Normalization processor によりスコアの平準化を行います。\n",
    "\n",
    "以下はハイブリッド検索の処理フローです\n",
    "\n",
    "<img src=\"./img/hybrid-search-overview.png\" width=\"1024\">\n",
    "\n",
    "[hybrid-search]: https://opensearch.org/docs/latest/search-plugins/hybrid-search/\n",
    "[normalization-processor]: https://opensearch.org/docs/latest/search-plugins/search-pipelines/normalization-processor/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7b8014",
   "metadata": {},
   "source": [
    "## 事前作業\n",
    "\n",
    "特になし"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2387e830",
   "metadata": {},
   "source": [
    "## ハイブリッド検索の実行\n",
    "\n",
    "### テキスト検索をベクトル検索で補う\n",
    "\n",
    "テキスト検索はクエリに厳密にマッチするドキュメントを取得可能です。<br>\n",
    "一方でクエリに厳密にマッチしないものの、意図としては近いドキュメントまでは拾うことができません。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddfeb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"jsquad-neural-search\"\n",
    "query = \"日本で梅雨がない地域は？\"\n",
    "\n",
    "payload = {\n",
    "  \"query\": {\n",
    "    \"match\": {\n",
    "      \"question\": {\n",
    "        \"query\": query,\n",
    "        \"operator\": \"and\"\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"_source\": False,\n",
    "  \"fields\": [\"question\", \"answers\", \"context\"],\n",
    "  \"size\": 10\n",
    "}\n",
    "response = opensearch_client.search(\n",
    "    index=index_name,\n",
    "    body=payload\n",
    ")\n",
    "pd.json_normalize(response[\"hits\"][\"hits\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fa91a6",
   "metadata": {},
   "source": [
    "他方、ベクトル検索は意味的に近いドキュメントを検索することに長けています"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1d2996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ニューラル検索用のパイプラインで代用する\n",
    "hybrid_search_pipeline_id = \"amazon.titan-embed-text-v2:0_neural_search_query\"\n",
    "\n",
    "index_name = \"jsquad-neural-search\"\n",
    "query = \"日本で梅雨がない地域は？\"\n",
    "\n",
    "payload = {\n",
    "  \"size\": 10,\n",
    "  \"query\": {\n",
    "    \"neural\": {\n",
    "      \"question_embedding\": {\n",
    "        \"query_text\": query, \n",
    "        \"k\": 10\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"_source\" : False,\n",
    "  \"fields\": [\"question\", \"answers\",  \"context\"]\n",
    "}\n",
    "response = opensearch_client.search(\n",
    "    index=index_name,\n",
    "    body=payload,\n",
    "    search_pipeline = hybrid_search_pipeline_id \n",
    ")\n",
    "pd.json_normalize(response[\"hits\"][\"hits\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea334c0",
   "metadata": {},
   "source": [
    "両者を組み合わせることで、意味的に近い検索結果もフォローしつつ、テキスト検索でマッチするドキュメントについてはよりスコアを上げる = 上位にランク付けすることが可能となります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3680ae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "index_name = \"jsquad-neural-search\"\n",
    "query = \"日本で梅雨がない地域は？\"\n",
    "\n",
    "payload = {\n",
    "  \"size\": 10,\n",
    "  \"query\": {\n",
    "    \"hybrid\": {\n",
    "      \"queries\": [\n",
    "        {\n",
    "          \"match\": {\n",
    "            \"question\": {\n",
    "              \"query\": query,\n",
    "              \"operator\": \"and\"\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        {\n",
    "          \"neural\": {\n",
    "            \"question_embedding\": {\n",
    "              \"query_text\": query, # テキストをベクトルに変換し\n",
    "              \"k\": 10 # クエリベクトルに近いベクトルのうち上位 10 件を返却\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  },\n",
    "  \"_source\" : False,\n",
    "  \"fields\": [\"question\", \"answers\",  \"context\"]\n",
    "}\n",
    "# 検索 API を実行\n",
    "response = opensearch_client.search(\n",
    "    body = payload,\n",
    "    index = index_name,\n",
    "    filter_path = \"hits.hits\",\n",
    "    search_pipeline = hybrid_search_pipeline_id \n",
    ")\n",
    "\n",
    "# 結果を表示\n",
    "pd.json_normalize(response[\"hits\"][\"hits\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36762190",
   "metadata": {},
   "source": [
    "ベクトル検索によって意図しない結果が付与されてしまう場合は、別のラボで解説するリランキングや、k ではなく min_score によるフィルタリングが有効です\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> \n",
    "AOSS では、`response_processors.rerank` が使用できないため、リランキングは保留にしました。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25694a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "index_name = \"jsquad-neural-search\"\n",
    "query = \"日本で梅雨がない地域は？\"\n",
    "payload = {\n",
    "  \"size\": 10,\n",
    "  \"query\": {\n",
    "    \"hybrid\": {\n",
    "      \"queries\": [\n",
    "        {\n",
    "          \"match\": {\n",
    "            \"question\": {\n",
    "              \"query\": query,\n",
    "              \"operator\": \"and\"\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        {\n",
    "          \"neural\": {\n",
    "            \"question_embedding\": {\n",
    "              \"query_text\": query, # テキストをベクトルに変換し\n",
    "              \"min_score\": 0.7\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  },\n",
    "  \"_source\" : False,\n",
    "  \"fields\": [\"question\", \"answers\",  \"context\"]\n",
    "}\n",
    "# 検索 API を実行\n",
    "response = opensearch_client.search(\n",
    "    body = payload,\n",
    "    index = index_name,\n",
    "    filter_path = \"hits.hits\",\n",
    "    search_pipeline = hybrid_search_pipeline_id \n",
    ")\n",
    "\n",
    "# 結果を表示\n",
    "pd.json_normalize(response[\"hits\"][\"hits\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa11dfee",
   "metadata": {},
   "source": [
    "### ベクトル検索をテキスト検索で補う\n",
    "\n",
    "ベクトル検索は意味的に近い文書を検索することに長けていますが、反面厳密なマッチングができないケースがあります。例えば、製品の型番などの業務固有のパラメーターでの検索は不得手です。\n",
    "\n",
    "以下のように \"M\" だけを検索対象としてみると、ベクトル検索は無関係の結果を返却します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eb09de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "index_name = \"jsquad-neural-search\"\n",
    "query = \"M\"\n",
    "payload = {\n",
    "  \"size\": 10,\n",
    "  \"query\": {\n",
    "    \"neural\": {\n",
    "      \"question_embedding\": {\n",
    "        \"query_text\": query, \n",
    "        \"k\": 10,\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"_source\" : False,\n",
    "  \"fields\": [\"question\", \"answers\",  \"context\"]\n",
    "}\n",
    "# 検索 API を実行\n",
    "response = opensearch_client.search(\n",
    "    body = payload,\n",
    "    index = index_name,\n",
    "    filter_path = \"hits.hits\",\n",
    "    search_pipeline = hybrid_search_pipeline_id \n",
    ")\n",
    "\n",
    "# 結果を表示\n",
    "pd.json_normalize(response[\"hits\"][\"hits\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3b146a",
   "metadata": {},
   "source": [
    "一方、テキスト検索の方は M を含む結果を返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab1b325",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"jsquad-neural-search\"\n",
    "query = \"M\"\n",
    "\n",
    "payload = {\n",
    "  \"query\": {\n",
    "    \"match\": {\n",
    "      \"question\": {\n",
    "        \"query\": query,\n",
    "        \"operator\": \"and\"\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"_source\": False,\n",
    "  \"fields\": [\"question\", \"answers\", \"context\"],\n",
    "  \"size\": 10\n",
    "}\n",
    "response = opensearch_client.search(\n",
    "    index=index_name,\n",
    "    body=payload\n",
    ")\n",
    "pd.json_normalize(response[\"hits\"][\"hits\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2be0ed2",
   "metadata": {},
   "source": [
    "ベクトル検索単体からハイブリッド検索に切り替えることで、検索精度の向上を達成できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958bdbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "index_name = \"jsquad-neural-search\"\n",
    "query = \"M\"\n",
    "payload = {\n",
    "  \"size\": 10,\n",
    "  \"query\": {\n",
    "    \"hybrid\": {\n",
    "      \"queries\": [\n",
    "        {\n",
    "          \"match\": {\n",
    "            \"question\": {\n",
    "              \"query\": query,\n",
    "              \"operator\": \"and\"\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        {\n",
    "          \"neural\": {\n",
    "            \"question_embedding\": {\n",
    "              \"query_text\": query, # テキストをベクトルに変換し\n",
    "              \"min_score\": 0.7\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  },\n",
    "  \"_source\" : False,\n",
    "  \"fields\": [\"question\", \"answers\",  \"context\"]\n",
    "}\n",
    "# 検索 API を実行\n",
    "response = opensearch_client.search(\n",
    "    body = payload,\n",
    "    index = index_name,\n",
    "    filter_path = \"hits.hits\",\n",
    "    search_pipeline = hybrid_search_pipeline_id \n",
    ")\n",
    "\n",
    "# 結果を表示\n",
    "pd.json_normalize(response[\"hits\"][\"hits\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d45ed0b",
   "metadata": {},
   "source": [
    "### テキスト検索とスパース検索のハイブリッド検索\n",
    "ベクトル検索の代わりに、スパース検索でテキスト検索を補完することも可能です。\n",
    "\n",
    "### ベクトル検索とスパース検索のハイブリッド検索\n",
    "ベクトル検索とスパース検索を組み合わせることも可能です。\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> \n",
    "東京リージョンの Bedrock では、スパース検索が可能なモデルは使用できないので、スパース検索は除外しました。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceee88a8",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "ハイブリッド検索で様々な検索を組み合わせられることを確認してきました。ハイブリッド検索は必ずしもベクトル検索との組み合わせが必須ではなく、テキスト検索同士の組み合わせも可能です。様々な場面での利用を検討してみてください。\n",
    "\n",
    "時間がある方は、続いて以下のラボも実施してみましょう。\n",
    "\n",
    "- ~~[セマンティックリランキング (Amazon SageMaker 編)](../reranking/semantic-reranking-with-sagemaker.ipynb)~~\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> \n",
    "AOSS では、`response_processors.rerank` が使用できないため、リランキングは保留にしました。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918b5706",
   "metadata": {},
   "source": [
    "## 後片付け\n",
    "\n",
    "### インデックス削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a0c55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"jsquad-hybrid-search\"\n",
    "\n",
    "try:\n",
    "    response = opensearch_client.indices.delete(index=index_name)\n",
    "    print(json.dumps(response, indent=2))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aoss-bedrock-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
